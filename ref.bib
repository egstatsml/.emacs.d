
@incollection{Bengio+chapter2007,
	title = {Scaling Learning Algorithms Towards {AI}},
	author = {Bengio, Yoshua and LeCun, Yann},
	year = 2007,
	booktitle = {Large Scale Kernel Machines},
	publisher = {MIT Press}
}

@article{Cortes1995,
	title = {Support-vector networks},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	year = 1995,
	month = {Sep},
	day = {01},
	journal = {Machine Learning},
	volume = 20,
	number = 3,
	pages = {273--297},
	doi = {10.1007/bf00994018},
	issn = {1573-0565},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.}
}

@article{GPflow2017,
	title = {{{GP}flow: A {G}aussian process library using {T}ensor{F}low}},
	author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke. and {Boukouvalas}, Alexis and {Le{\'o}n-Villagr{\'a}}, Pablo and Ghahramani, Zoubin and Hensman, James},
	year = 2017,
	month = 4,
	journal = {Journal of Machine Learning Research},
	volume = 18,
	number = 40,
	pages = {1--6},
	url = {http://jmlr.org/papers/v18/16-537.html}
}

@online{Guynn2015,
	title = {Google Photos labeled black people 'gorillas'},
	author = {Guynn, Jessica},
	url = {https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/},
	date = {2015-07-01},
	editor = {{USA Today}}
}

@article{Hinton06,
	title = {A Fast Learning Algorithm for Deep Belief Nets},
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
	year = 2006,
	journal = {Neural Computation},
	volume = 18,
	pages = {1527--1554}
}

@article{McCulloch1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = 1943,
	month = {Dec},
	day = {01},
	journal = {The bulletin of mathematical biophysics},
	volume = 5,
	number = 4,
	pages = {115--133},
	doi = {10.1007/bf02478259},
	issn = {1522-9602},
	abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.}
}

@online{abc2018,
	title = {Uber suspends self-driving car tests after vehicle hits and kills woman crossing the street in Arizona},
	author = {{ABC News}},
	year = 2018,
	url = {http://www.abc.net.au/news/2018-03-20/uber-suspends-self-driving-car-tests-after-fatal-crash/9565586},
	date = {2018-03-20}
}

@article{ahn2012bayesian,
	title = {Bayesian posterior sampling via stochastic gradient Fisher scoring},
	author = {Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
	year = 2012,
	journal = {arXiv preprint arXiv:1206.6380}
}

@book{amari2012differential,
	title = {Differential-geometrical methods in statistics},
	author = {Amari, Shun-ichi},
	year = 2012,
	publisher = {Springer Science \& Business Media},
	volume = 28
}

@article{araujo2017classification,
	title = {Classification of breast cancer histology images using convolutional neural networks},
	author = {Ara{\'u}jo, Teresa and Aresta, Guilherme and Castro, Eduardo and Rouco, Jos{\'e} and Aguiar, Paulo and Eloy, Catarina and Pol{\'o}nia, Ant{\'o}nio and Campilho, Aur{\'e}lio},
	year = 2017,
	journal = {PloS one},
	publisher = {Public Library of Science},
	volume = 12,
	number = 6,
	pages = {e0177544}
}

@article{athey2015machine,
	title = {Machine learning methods for estimating heterogeneous causal effects},
	author = {Athey, Susan and Imbens, Guido W},
	year = 2015,
	journal = {stat},
	volume = 1050,
	number = 5
}

@article{barber1998ensemble,
	title = {Ensemble learning in Bayesian neural networks},
	author = {Barber, David and Bishop, Christopher M},
	year = 1998,
	journal = {Nato ASI Series F Computer and Systems Sciences},
	publisher = {Springer Verlag},
	volume = 168,
	pages = {215--238}
}

@book{bernardo2000,
	title = {Bayesian Theory},
	author = {Jos√© M. Bernardo and Adrian F. M. Smith},
	year = 2000,
	publisher = {John Wiley \& Sons}
}

@article{bertazzi2020adaptive,
	title = {Adaptive Schemes for Piecewise Deterministic Monte Carlo Algorithms},
	author = {Bertazzi, Andrea and Bierkens, Joris},
	year = 2020,
	journal = {arXiv preprint arXiv:2012.13924}
}

@inproceedings{betancourt2015fundamental,
	title = {The fundamental incompatibility of scalable Hamiltonian Monte Carlo and naive data subsampling},
	author = {Betancourt, Michael},
	year = 2015,
	booktitle = {International Conference on Machine Learning},
	pages = {533--540}
}

@article{betancourt2017conceptual,
	title = {A conceptual introduction to Hamiltonian Monte Carlo},
	author = {Betancourt, Michael},
	year = 2017,
	journal = {arXiv preprint arXiv:1701.02434}
}

@article{betancourt2017geometric,
	title = {The geometric foundations of hamiltonian monte carlo},
	author = {Betancourt, Michael and Byrne, Simon and Livingstone, Sam and Girolami, Mark and others},
	year = 2017,
	journal = {Bernoulli},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	volume = 23,
	number = {4a},
	pages = {2257--2298}
}

@article{bierkens2016non,
	title = {Non-reversible metropolis-hastings},
	author = {Bierkens, Joris},
	year = 2016,
	journal = {Statistics and Computing},
	publisher = {Springer},
	volume = 26,
	number = 6,
	pages = {1213--1228}
}

@article{bierkens2017limit,
	title = {Limit theorems for the zig-zag process},
	author = {Bierkens, Joris and Duncan, Andrew},
	year = 2017,
	journal = {Advances in Applied Probability},
	publisher = {Cambridge University Press},
	volume = 49,
	number = 3,
	pages = {791--825}
}

@article{bierkens2017piecewise,
	title = {A piecewise deterministic scaling limit of lifted Metropolis--Hastings in the Curie--Weiss model},
	author = {Bierkens, Joris and Roberts, Gareth and others},
	year = 2017,
	journal = {The Annals of Applied Probability},
	publisher = {Institute of Mathematical Statistics},
	volume = 27,
	number = 2,
	pages = {846--882}
}

@article{bierkens2019ergodicity,
	title = {Ergodicity of the zigzag process},
	author = {Bierkens, Joris and Roberts, Gareth O and Zitt, Pierre-Andr{\'e} and others},
	year = 2019,
	journal = {The Annals of Applied Probability},
	publisher = {Institute of Mathematical Statistics},
	volume = 29,
	number = 4,
	pages = {2266--2301}
}

@article{bierkens2019zig,
	title = {The zig-zag process and super-efficient sampling for Bayesian analysis of big data},
	author = {Bierkens, Joris and Fearnhead, Paul and Roberts, Gareth and others},
	year = 2019,
	journal = {The Annals of Statistics},
	publisher = {Institute of Mathematical Statistics},
	volume = 47,
	number = 3,
	pages = {1288--1320}
}

@article{bierkens2020boomerang,
	title = {The Boomerang Sampler},
	author = {Bierkens, Joris and Grazzi, Sebastiano and Kamatani, Kengo and Roberts, Gareth},
	year = 2020,
	journal = {arXiv preprint arXiv:2006.13777}
}

@book{bishop2006,
	title = {Pattern recognition and machine learning},
	author = {Bishop, C.},
	year = 2006,
	publisher = {Springer},
	address = {New York}
}

@article{blei2017variational,
	title = {Variational inference: A review for statisticians},
	author = {Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
	year = 2017,
	journal = {Journal of the American statistical Association},
	publisher = {Taylor \& Francis},
	volume = 112,
	number = 518,
	pages = {859--877}
}

@article{blundell2015weight,
	title = {Weight uncertainty in neural networks},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	year = 2015,
	journal = {arXiv preprint arXiv:1505.05424}
}

@article{bouchard2018bouncy,
	title = {The bouncy particle sampler: A nonreversible rejection-free Markov chain Monte Carlo method},
	author = {Bouchard-C{\^o}t{\'e}, Alexandre and Vollmer, Sebastian J and Doucet, Arnaud},
	year = 2018,
	journal = {Journal of the American Statistical Association},
	publisher = {Taylor \& Francis},
	volume = 113,
	number = 522,
	pages = {855--867}
}

@book{brooks2011handbook,
	title = {Handbook of markov chain monte carlo},
	author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	year = 2011,
	publisher = {CRC press}
}

@inproceedings{brosse2018promises,
	title = {The promises and pitfalls of stochastic gradient Langevin dynamics},
	author = {Brosse, Nicolas and Durmus, Alain and Moulines, Eric},
	year = 2018,
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {8268--8278}
}

@inproceedings{buolamwini2018gender,
	title = {Gender shades: Intersectional accuracy disparities in commercial gender classification},
	author = {Buolamwini, Joy and Gebru, Timnit},
	year = 2018,
	booktitle = {Conference on fairness, accountability and transparency},
	pages = {77--91}
}

@article{campbell2019automated,
	title = {Automated Scalable Bayesian Inference via Hilbert Coresets},
	author = {Trevor Campbell and Tamara Broderick},
	year = 2019,
	journal = {Journal of Machine Learning Research},
	volume = 20,
	number = 15,
	pages = {1--38},
	url = {http://jmlr.org/papers/v20/17-613.html}
}

@inproceedings{caruana2015intelligible,
	title = {Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},
	author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
	year = 2015,
	booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {1721--1730},
	organization = {Acm}
}

@inproceedings{chen1999lifting,
	title = {Lifting Markov chains to speed up mixing},
	author = {Chen, Fang and Lov{\'a}sz, L{\'a}szl{\'o} and Pak, Igor},
	year = 1999,
	booktitle = {Stoc'99},
	organization = {Citeseer}
}

@book{chen2012monte,
	title = {Monte Carlo methods in Bayesian computation},
	author = {Chen, Ming-Hui and Shao, Qi-Man and Ibrahim, Joseph G},
	year = 2012,
	publisher = {Springer Science \& Business Media}
}

@inproceedings{chen2014hmc,
	title = {Stochastic Gradient Hamiltonian Monte Carlo},
	author = {Tianqi Chen and Emily Fox and Carlos Guestrin},
	year = 2014,
	month = {22--24 Jun},
	booktitle = {Proceedings of the 31st International Conference on Machine Learning},
	publisher = {Pmlr},
	address = {Bejing, China},
	series = {Proceedings of Machine Learning Research},
	volume = 32,
	number = 2,
	pages = {1683--1691},
	url = {http://proceedings.mlr.press/v32/cheni14.html},
	editor = {Eric P. Xing and Tony Jebara},
	pdf = {http://proceedings.mlr.press/v32/cheni14.pdf},
	abstract = {Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals.  The popularity of such methods has grown significantly in recent years.  However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data.  In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad.  To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution.  Results on simulated data validate our theory.  We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icml/ChenFG14.bib},
	timestamp = {Wed, 29 May 2019 01:00:00 +0200}
}

@inproceedings{chen2014stochastic,
	title = {Stochastic gradient hamiltonian monte carlo},
	author = {Chen, Tianqi and Fox, Emily and Guestrin, Carlos},
	year = 2014,
	booktitle = {International conference on machine learning},
	pages = {1683--1691}
}

@incollection{chen2015complete,
	title = {A Complete Recipe for Stochastic Gradient MCMC},
	author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily},
	year = 2015,
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	pages = {2917--2925},
	url = {http://papers.nips.cc/paper/5891-a-complete-recipe-for-stochastic-gradient-mcmc.pdf},
	editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett}
}

@article{chen2016thinning,
	title = {Thinning algorithms for simulating point processes},
	author = {Chen, Yuanda},
	journal = {Florida State University, Tallahassee, FL},
	year = {2016}
}

@misc{chollet2015,
	title = {keras},
	author = {Fran√ßois Chollet},
	year = 2015,
	journal = {GitHub repository},
	publisher = {GitHub},
	howpublished = {\url{https://github.com/fchollet/keras}},
	commit = {5bcac37}
}

@article{claudiu2010,
	title = {Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition},
	author = {Dan Claudiu Ciresan and Ueli Meier and Luca Maria Gambardella and J{\"{u}}rgen Schmidhuber},
	year = 2010,
	journal = {CoRR},
	volume = {abs/1003.0358},
	url = {http://arxiv.org/abs/1003.0358},
	archiveprefix = {arXiv},
	eprint = {1003.0358},
	timestamp = {Mon, 13 Aug 2018 16:47:03 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1003-0358},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{clevert2015fast,
	title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
	author = {Djork{-}Arn{\'{e}} Clevert and Thomas Unterthiner and Sepp Hochreiter},
	year = 2016,
	booktitle = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
	url = {http://arxiv.org/abs/1511.07289},
	editor = {Yoshua Bengio and Yann LeCun},
	timestamp = {Sat, 23 Jan 2021 01:12:05 +0100},
	biburl = {https://dblp.org/rec/journals/corr/ClevertUH15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cremer2018inference,
	title = {Inference Suboptimality in Variational Autoencoders},
	author = {Chris Cremer and Xuechen Li and David K. Duvenaud},
	year = 2018,
	journal = {CoRR},
	volume = {abs/1801.03558},
	url = {http://arxiv.org/abs/1801.03558},
	archiveprefix = {arXiv},
	eprint = {1801.03558},
	timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1801-03558},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cybenko1989approximation,
	title = {Approximation by superpositions of a sigmoidal function},
	author = {Cybenko, George},
	year = 1989,
	journal = {Mathematics of control, signals and systems},
	publisher = {Springer},
	volume = 2,
	number = 4,
	pages = {303--314}
}

@article{dahl2012context,
	title = {Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition},
	author = {Dahl, George E and Yu, Dong and Deng, Li and Acero, Alex},
	year = 2012,
	journal = {IEEE Transactions on audio, speech, and language processing},
	publisher = {Ieee},
	volume = 20,
	number = 1,
	pages = {30--42}
}

@inproceedings{damianou2011variational,
	title = {Variational Gaussian process dynamical systems},
	author = {Damianou, Andreas and Titsias, Michalis K and Lawrence, Neil D},
	year = 2011,
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {2510--2518}
}

@inproceedings{damianou2013deep,
	title = {Deep gaussian processes},
	author = {Damianou, Andreas and Lawrence, Neil},
	year = 2013,
	booktitle = {Artificial Intelligence and Statistics},
	pages = {207--215}
}

@phdthesis{damianou2015deep,
	title = {Deep Gaussian processes and variational propagation of uncertainty},
	author = {Damianou, Andreas},
	year = 2015,
	school = {University of Sheffield}
}

@article{davis1984piecewise,
	title = {Piecewise-deterministic Markov processes: a general class of non-diffusion stochastic models},
	author = {Davis, Mark HA},
	year = 1984,
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	publisher = {Wiley Online Library},
	volume = 46,
	number = 3,
	pages = {353--376}
}

@book{davis1993markov,
	title = {Markov models \& optimization},
	author = {Davis, Mark HA},
	year = 1993,
	publisher = {CRC Press},
	volume = 49
}

@article{deligiannidis2019exponential,
	title = {Exponential ergodicity of the bouncy particle sampler},
	author = {Deligiannidis, George and Bouchard-C{\^o}t{\'e}, Alexandre and Doucet, Arnaud and others},
	year = 2019,
	journal = {The Annals of Statistics},
	publisher = {Institute of Mathematical Statistics},
	volume = 47,
	number = 3,
	pages = {1268--1287}
}

@inproceedings{deng2009imagenet,
	title = {Imagenet: A large-scale hierarchical image database},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = 2009,
	booktitle = {Computer Vision and Pattern Recognition},
	pages = {248--255},
	organization = {Ieee}
}

@inproceedings{denker1991transforming,
	title = {Transforming neural-net output levels to probability distributions},
	author = {Denker, John S and Lecun, Yann},
	year = 1991,
	booktitle = {Advances in neural information processing systems},
	pages = {853--859}
}

@article{denton2021geneology,
	title = {On the genealogy of machine learning datasets: A critical history of ImageNet},
	author = {Emily Denton and Alex Hanna and Razvan Amironesei and Andrew Smart and Hilary Nicole},
	year = 2021,
	journal = {Big Data \& Society},
	volume = 8,
	number = 2,
	pages = 20539517211035955,
	doi = {10.1177/20539517211035955},
	url = {https://doi.org/10.1177/20539517211035955},
	eprint = {https://doi.org/10.1177/20539517211035955},
	abstract = {In response to growing concerns of bias, discrimination, and unfairness perpetuated by algorithmic systems, the datasets used to train and evaluate machine learning models have come under increased scrutiny. Many of these examinations have focused on the contents of machine learning datasets, finding glaring underrepresentation of minoritized groups. In contrast, relatively little work has been done to examine the norms, values, and assumptions embedded in these datasets. In this work, we conceptualize machine learning datasets as a type of informational infrastructure, and motivate a genealogy as method in examining the histories and modes of constitution at play in their creation. We present a critical history of ImageNet as an exemplar, utilizing critical discourse analysis of major texts around ImageNet‚Äôs creation and impact. We find that assumptions around ImageNet and other large computer vision datasets more generally rely on three themes: the aggregation and accumulation of more data, the computational construction of meaning, and making certain types of data labor invisible. By tracing the discourses that surround this influential benchmark, we contribute to the ongoing development of the standards and norms around data development in machine learning and artificial intelligence research.}
}

@article{devroye2006nonuniform,
	title = {Nonuniform random variate generation},
	author = {Devroye, Luc},
	year = 2006,
	journal = {Handbooks in operations research and management science},
	publisher = {Elsevier},
	volume = 13,
	pages = {83--121}
}

@article{diaconis2000analysis,
	title = {Analysis of a nonreversible Markov chain sampler},
	author = {Diaconis, Persi and Holmes, Susan and Neal, Radford M},
	year = 2000,
	journal = {Annals of Applied Probability},
	publisher = {Jstor},
	pages = {726--752}
}

@article{dillon2017tensorflow,
	title = {Tensorflow distributions},
	author = {Dillon, Joshua V and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A},
	year = 2017,
	journal = {arXiv preprint arXiv:1711.10604}
}

@article{dinh2016density,
	title = {Density estimation using Real {NVP}},
	author = {Laurent Dinh and Jascha Sohl{-}Dickstein and Samy Bengio},
	year = 2016,
	journal = {CoRR},
	volume = {abs/1605.08803},
	url = {http://arxiv.org/abs/1605.08803},
	archiveprefix = {arXiv},
	eprint = {1605.08803},
	timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/DinhSB16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{duane1987hybrid,
	title = {Hybrid monte carlo},
	author = {Duane, Simon and Kennedy, Anthony D and Pendleton, Brian J and Roweth, Duncan},
	year = 1987,
	journal = {Physics letters B},
	publisher = {Elsevier},
	volume = 195,
	number = 2,
	pages = {216--222}
}

@article{erhan2009visualizing,
	title = {Visualizing higher-layer features of a deep network},
	author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	year = 2009,
	journal = {University of Montreal},
	volume = 1341,
	number = 3,
	pages = 1
}

@article{esteva2017dermatologist,
	title = {Dermatologist-level classification of skin cancer with deep neural networks},
	author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
	year = 2017,
	journal = {Nature},
	publisher = {Nature Publishing Group},
	volume = 542,
	number = 7639,
	pages = 115
}

@misc{eu2016,
	title = {Regulation (EU) 2016/679 of the European Parliment and of the Council},
	author = {Council{\ }of{\ }European{\ }Union},
	year = 2016
}

@article{fearnhead2018piecewise,
	title = {Piecewise deterministic Markov processes for continuous-time Monte Carlo},
	author = {Fearnhead, Paul and Bierkens, Joris and Pollock, Murray and Roberts, Gareth O and others},
	year = 2018,
	journal = {Statistical Science},
	publisher = {Institute of Mathematical Statistics},
	volume = 33,
	number = 3,
	pages = {386--412}
}

@article{fei2007learning,
	title = {Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
	author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
	year = 2007,
	journal = {Computer vision and Image understanding},
	publisher = {Elsevier},
	volume = 106,
	number = 1,
	pages = {59--70}
}

@misc{freidank19:_arspy,
	title = {ARSpy},
	author = {Moritz Freidank and Matt Graham},
	year = 2019,
	url = {https://github.com/MFreidank/ARSpy}
}

@article{funahashi1989approximate,
	title = {On the approximate realization of continuous mappings by neural networks},
	author = {Funahashi, Ken-Ichi},
	year = 1989,
	journal = {Neural networks},
	publisher = {Pergamon},
	volume = 2,
	number = 3,
	pages = {183--192}
}

@article{gal2015dropoutAppendix,
	title = {Dropout as a Bayesian Approximation: Appendix},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	year = 2015,
	eprint = {1506.02157},
	archiveprefix = {arXiv}
}

@inproceedings{gal2015dropoutslides,
	title = {Dropout as a Bayesian approximation: Insights and applications},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	year = 2015,
	booktitle = {Deep Learning Workshop, ICML},
	volume = 1,
	pages = 2
}

@inproceedings{gal2016dropout,
	title = {Dropout as a Bayesian approximation: Representing model uncertainty in deep learning},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	year = 2016,
	booktitle = {international conference on machine learning},
	publisher = {JMLR.org},
	series = {{JMLR} Workshop and Conference Proceedings},
	volume = 48,
	pages = {1050--1059},
	url = {http://proceedings.mlr.press/v48/gal16.html},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icml/GalG16.bib},
	editor = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
	timestamp = {Wed, 29 May 2019 01:00:00 +0200}
}

@article{gal2016uncertainty,
	title = {Uncertainty in deep learning},
	author = {Gal, Yarin},
	year = 2016,
	journal = {University of Cambridge}
}

@inproceedings{gal2017concrete,
	title = {Concrete dropout},
	author = {Gal, Yarin and Hron, Jiri and Kendall, Alex},
	year = 2017,
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {3581--3590}
}

@article{garriga2018deep,
	title = {Deep convolutional networks as shallow gaussian processes},
	author = {Garriga-Alonso, Adri{\`a} and Aitchison, Laurence and Rasmussen, Carl Edward},
	year = 2018,
	journal = {arXiv preprint arXiv:1808.05587}
}

@article{gilks1992adaptive,
	title = {Adaptive rejection sampling for Gibbs sampling},
	author = {Gilks, Walter R and Wild, Pascal},
	year = 1992,
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	publisher = {Wiley Online Library},
	volume = 41,
	number = 2,
	pages = {337--348}
}

@article{gilks1995adaptive,
	title = {Adaptive rejection Metropolis sampling within Gibbs sampling},
	author = {Gilks, Wally R and Best, Nicky G and Tan, KKC},
	year = 1995,
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	publisher = {Wiley Online Library},
	volume = 44,
	number = 4,
	pages = {455--472}
}

@article{giordana2018covariances,
	title = {Covariances, Robustness, and Variational Bayes},
	author = {Ryan Giordano and Tamara Broderick and Michael I. Jordan},
	year = 2018,
	journal = {Journal of Machine Learning Research},
	volume = 19,
	number = 51,
	pages = {1--49},
	url = {http://jmlr.org/papers/v19/17-670.html}
}

@article{girolami2011riemann,
	title = {Riemann manifold langevin and hamiltonian monte carlo methods},
	author = {Girolami, Mark and Calderhead, Ben},
	year = 2011,
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	publisher = {Wiley Online Library},
	volume = 73,
	number = 2,
	pages = {123--214}
}

@inproceedings{girshick2014rich,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = 2014,
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {580--587}
}

@inproceedings{glorot2011deep,
	title = {Deep sparse rectifier neural networks},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	year = 2011,
	booktitle = {Aistats},
	pages = {315--323}
}

@incollection{goan2019bayesian,
	title = {Bayesian Neural Networks: an Introduction and Review},
	author = {Goan, E.},
	year = {in preparation},
	booktitle = {Studies in Bayesian Modelling and Analysis of Big Data},
	publisher = {Springer},
	editor = {Mengersen, K. and Pudlo, P. and Robert. C}
}

@inproceedings{goodfellow2013maxout,
	title = {Maxout networks},
	author = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	year = 2013,
	booktitle = {International conference on machine learning},
	pages = {1319--1327},
	organization = {Pmlr}
}

@book{goodfellow2016deep,
	title = {Deep learning},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	year = 2016,
	publisher = {MIT Press},
	volume = 1
}

@article{goodman2016european,
	title = {European Union regulations on algorithmic decision-making and a" right to explanation"},
	author = {Goodman, Bryce and Flaxman, Seth},
	year = 2016,
	eprint = {1606.08813},
	archiveprefix = {arXiv}
}

@article{grant2009lit,
	title = {A typology of reviews: an analysis of 14 review types and associated methodologies},
	author = {Maria J. Grant and Andrew Booth},
	journal = {Health Information \& Libraries Journal},
	volume = 26,
	number = 2,
	pages = {91--108},
	doi = {10.1111/j.1471-1842.2009.00848.x},
	abstract = {Abstract Background and objectives:‚ÄÇ The expansion of evidence‚Äêbased practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. The objective of this study is to provide descriptive insight into the most common types of reviews, with illustrative examples from health and health information domains. Methods:‚ÄÇ Following scoping searches, an examination was made of the vocabulary associated with the literature of review and synthesis (literary warrant). A simple analytical framework‚ÄîSearch, AppraisaL, Synthesis and Analysis (SALSA)‚Äîwas used to examine the main review types. Results:‚ÄÇ Fourteen review types and associated methodologies were analysed against the SALSA framework, illustrating the inputs and processes of each review type. A description of the key characteristics is given, together with perceived strengths and weaknesses. A limited number of review types are currently utilized within the health information domain. Conclusions:‚ÄÇ Few review types possess prescribed and explicit methodologies and many fall short of being mutually exclusive. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain.}
}

@incollection{graves2011,
	title = {Practical Variational Inference for Neural Networks},
	author = {Graves, Alex},
	year = 2011,
	booktitle = {Advances in Neural Information Processing Systems 24},
	publisher = {Curran Associates, Inc.},
	pages = {2348--2356},
	editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger}
}

@inproceedings{graves2011practical,
	title = {Practical Variational Inference for Neural Networks},
	author = {Alex Graves},
	year = 2011,
	booktitle = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain},
	pages = {2348--2356},
	url = {https://proceedings.neurips.cc/paper/2011/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/Graves11.bib},
	editor = {John Shawe{-}Taylor and Richard S. Zemel and Peter L. Bartlett and Fernando C. N. Pereira and Kilian Q. Weinberger},
	timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@report{griffin2007caltech,
	title = {Caltech-256 object category dataset},
	author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
	year = 2007,
	type = {Technical Report 7694},
	institution = {California Institute of Technology}
}

@article{gull1999quantified,
	title = {Quantified Maximum Entropy MemSys5 Users‚Äô Manual},
	author = {Gull, Stephen F and Skilling, John},
	year = 1991,
	journal = {Maximum Entropy Data Consultants Ltd},
	volume = 33
}

@article{gunning2017explainable,
	title = {Explainable artificial intelligence (xai)},
	author = {Gunning, David},
	year = 2017,
	journal = {Defense Advanced Research Projects Agency (DARPA), nd Web}
}

@article{guo21_beyon_self_atten,
	title = {Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks},
	author = {Guo, Meng-Hao and Liu, Zheng-Ning and Mu, Tai-Jiang and Hu, Shi-Min},
	year = 2021,
	journal = {CoRR},
	url = {http://arxiv.org/abs/2105.02358v2},
	abstract = {Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification.  Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.},
	archiveprefix = {arXiv},
	eprint = {2105.02358},
	primaryclass = {cs.CV}
}

@article{hafner2018reliable,
	title = {Reliable uncertainty estimates in deep neural networks using noise contrastive priors},
	author = {Hafner, Danijar and Tran, Dustin and Lillicrap, Timothy and Irpan, Alex and Davidson, James},
	year = 2018
}

@article{heek2019bayesian,
	title = {Bayesian inference for large scale image classification},
	author = {Heek, Jonathan and Kalchbrenner, Nal},
	year = 2019,
	journal = {arXiv preprint arXiv:1908.03491}
}

@inproceedings{hernandez2015probabilistic,
	title = {Probabilistic backpropagation for scalable learning of bayesian neural networks},
	author = {Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
	year = 2015,
	booktitle = {International Conference on Machine Learning},
	pages = {1861--1869}
}

@inproceedings{hinton1993keeping,
	title = {Keeping the neural networks simple by minimizing the description length of the weights},
	author = {Hinton, Geoffrey E and Van Camp, Drew},
	year = 1993,
	booktitle = {Proceedings of the sixth annual conference on Computational learning theory},
	pages = {5--13}
}

@article{hinton2012deep,
	title = {Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
	author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
	year = 2012,
	journal = {IEEE Signal Processing Magazine},
	publisher = {Ieee},
	volume = 29,
	number = 6,
	pages = {82--97}
}

@article{hoffman2013stochastic,
	title = {Stochastic variational inference},
	author = {Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
	year = 2013,
	journal = {The Journal of Machine Learning Research},
	publisher = {JMLR. org},
	volume = 14,
	number = 1,
	pages = {1303--1347}
}

@article{holland1985statistics,
	title = {Statistics and causal inference},
	author = {Holland, Paul W and Glymour, Clark and Granger, Clive},
	year = 1985,
	journal = {ETS Research Report Series},
	publisher = {Wiley Online Library},
	volume = 1985,
	number = 2
}

@misc{holzinger2017we,
	title = {What do we need to build explainable AI systems for the medical domain?},
	author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S and Kell, Douglas B},
	year = 2017
}

@article{hornik1991approximation,
	title = {Approximation capabilities of multilayer feedforward networks},
	author = {Hornik, Kurt},
	year = 1991,
	journal = {Neural networks},
	publisher = {Elsevier},
	volume = 4,
	number = 2,
	pages = {251--257}
}

@incollection{huggins2016core,
	title = {Coresets for Scalable Bayesian Logistic Regression},
	author = {Huggins, Jonathan and Campbell, Trevor and Broderick, Tamara},
	year = 2016,
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	pages = {4080--4088},
	url = {http://papers.nips.cc/paper/6486-coresets-for-scalable-bayesian-logistic-regression.pdf},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}

@article{izmailov2021bayesian,
	title = {What Are Bayesian Neural Network Posteriors Really Like?},
	author = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D and Wilson, Andrew Gordon},
	journal = {arXiv preprint arXiv:2104.14421},
	year = {2021}
}

@incollection{jaakkola1998improving,
	title = {Improving the mean field approximation via the use of mixture distributions},
	author = {Jaakkola, Tommi S and Jordan, Michael I},
	year = 1998,
	booktitle = {Learning in graphical models},
	publisher = {Springer},
	pages = {163--173}
}

@article{jia2014caffe,
	title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year = 2014,
	journal = {arXiv preprint arXiv:1408.5093}
}

@article{jordan1999introduction,
	title = {An introduction to variational methods for graphical models},
	author = {Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
	year = 1999,
	journal = {Machine learning},
	publisher = {Springer},
	volume = 37,
	number = 2,
	pages = {183--233}
}

@phdthesis{kim2015interactive,
	title = {Interactive and interpretable machine learning models for human machine collaboration},
	author = {Kim, Been},
	year = 2015,
	school = {Massachusetts Institute of Technology}
}

@article{kindermans2017learning,
	title = {Learning how to explain neural networks: PatternNet and PatternAttribution},
	author = {Kindermans, Pieter-Jan and Sch{\"u}tt, Kristof T and Alber, Maximilian and M{\"u}ller, Klaus-Robert and Erhan, Dumitru and Kim, Been and D{\"a}hne, Sven},
	year = 2017,
	eprint = {1705.05598},
	archiveprefix = {arXiv}
}

@article{kingma2013auto,
	title = {Auto-encoding variational bayes},
	author = {Kingma, Diederik P and Welling, Max},
	year = 2013,
	eprint = {1312.6114},
	archiveprefix = {arXiv}
}

@inproceedings{kingma2014auto,
	author = {Diederik P. Kingma and
               Max Welling},
	editor = {Yoshua Bengio and
               Yann LeCun},
	title = {Auto-Encoding Variational Bayes},
	booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
	year = {2014},
	url = {http://arxiv.org/abs/1312.6114},
	timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
	biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kingma2015variational,
	title = {Variational dropout and the local reparameterization trick},
	author = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
	year = 2015,
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {2575--2583},
	url = {https://proceedings.neurips.cc/paper/2015/hash/bc7316929fe1545bf0b98d114ee3ecb8-Abstract.html},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/BlumHP15.bib},
	editor = {Corinna Cortes and Neil D. Lawrence and Daniel D. Lee and Masashi Sugiyama and Roman Garnett},
	timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{kohavi1996scaling,
	title = {Scaling up the accuracy of Naive-Bayes classifiers: a decision-tree hybrid.},
	author = {Kohavi, Ron},
	year = 1996,
	booktitle = {Kdd},
	volume = 96,
	pages = {202--207},
	organization = {Citeseer}
}

@phdthesis{krak2016,
	title = {Building Interpretable Models: From Bayesian Networks to Neural Networks},
	author = {Krakovna,Viktoriya},
	year = 2016,
	journal = {ProQuest Dissertations and Theses},
	pages = 85,
	isbn = 9780355304961,
	keywords = {Pure sciences; Applied sciences; Interpretability; Recurrent neural networks; Variable selection; Visualization; Statistics; Computer science; 0463:Statistics; 0984:Computer science},
	language = {English}
}

@article{krizhevsky2009learning,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex and Hinton, Geoffrey},
	year = 2009,
	publisher = {Citeseer}
}

@inproceedings{krizhevsky2012imagenet,
	title = {Imagenet classification with deep convolutional neural networks},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = 2012,
	booktitle = {Advances in neural information processing systems},
	pages = {1097--1105}
}

@article{kucukelbir2016automatic,
	title = {{Automatic Differentiation Variational Inference}},
	author = {{Kucukelbir}, Alp and {Tran}, Dustin and {Ranganath}, Rajesh and {Gelman}, Andrew and {Blei}, David M.},
	year = 2016,
	month = {Mar},
	journal = {arXiv e-prints},
	pages = {arXiv:1603.00788},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation},
	eid = {arXiv:1603.00788},
	archiveprefix = {arXiv},
	eprint = {1603.00788},
	primaryclass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160300788K},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{kuleshov2018accurate,
	title = {Accurate uncertainties for deep learning using calibrated regression},
	author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
	year = 2018,
	journal = {arXiv preprint arXiv:1807.00263}
}

@article{lampinen2001bayesian,
	title = {Bayesian approach for neural networks‚Äîreview and case studies},
	author = {Lampinen, Jouko and Vehtari, Aki},
	year = 2001,
	journal = {Neural networks},
	publisher = {Elsevier},
	volume = 14,
	number = 3,
	pages = {257--274}
}

@online{lawrence2019deep,
	title = {Deep Gaussian Processes},
	author = {Neil Lawrence},
	year = 2019,
	url = {http://inverseprobability.com/talks/notes/deep-gaussian-processes.html},
	date = {01/04/2019}
}

@article{lecun1989backpropagation,
	title = {Backpropagation applied to handwritten zip code recognition},
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
	year = 1989,
	journal = {Neural computation},
	publisher = {MIT Press},
	volume = 1,
	number = 4,
	pages = {541--551}
}

@article{lecun1998gradient,
	title = {Gradient-based learning applied to document recognition},
	author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	year = 1998,
	journal = {Proceedings of the IEEE},
	publisher = {Ieee},
	volume = 86,
	number = 11,
	pages = {2278--2324}
}

@article{lecun1998mnist,
	title = {Gradient-based learning applied to document recognition},
	author = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
	year = 1998,
	month = {Nov},
	journal = {Proceedings of the IEEE},
	volume = 86,
	number = 11,
	pages = {2278--2324},
	doi = {10.1109/5.726791},
	issn = {0018-9219},
	keywords = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis}
}

@article{lecun2015deep,
	title = {Deep learning},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = 2015,
	journal = {nature},
	publisher = {Nature Publishing Group},
	volume = 521,
	number = 7553,
	pages = 436
}

@inproceedings{lee2018deep,
	title = {Deep Neural Networks as Gaussian Processes},
	author = {Jaehoon Lee and Jascha Sohl-dickstein and Jeffrey Pennington and Roman Novak and Sam Schoenholz and Yasaman Bahri},
	year = 2018,
	booktitle = {International Conference on Learning Representations},
	url = {https://openreview.net/forum?id=B1EA-M-0Z}
}

@article{letham2015,
	title = {Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model},
	author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
	year = 2015,
	month = {09},
	journal = {Ann. Appl. Stat.},
	publisher = {The Institute of Mathematical Statistics},
	volume = 9,
	number = 3,
	pages = {1350--1371},
	fjournal = {The Annals of Applied Statistics}
}

@book{levin2017markov,
	title = {Markov chains and mixing times},
	author = {Levin, David A and Peres, Yuval},
	year = 2017,
	publisher = {American Mathematical Soc.},
	volume = 107
}

@article{lewis1979simulation,
	title = {Simulation of nonhomogeneous Poisson processes by thinning},
	author = {Lewis, PA W and Shedler, Gerald S},
	year = 1979,
	journal = {Naval research logistics quarterly},
	publisher = {Wiley Online Library},
	volume = 26,
	number = 3,
	pages = {403--413}
}

@article{li2015preconditioned,
	title = {Preconditioned stochastic gradient Langevin dynamics for deep neural networks},
	author = {Li, Chunyuan and Chen, Changyou and Carlson, David and Carin, Lawrence},
	year = 2015,
	journal = {arXiv preprint arXiv:1512.07666}
}

@article{lipton2016mythos,
	title = {The mythos of model interpretability},
	author = {Lipton, Zachary C},
	year = 2016,
	journal = {CoRR},
	eprint = {1606.03490},
	archiveprefix = {arXiv}
}

@article{livnat2010sex,
	title = {Sex, mixability, and modularity},
	author = {Livnat, Adi and Papadimitriou, Christos and Pippenger, Nicholas and Feldman, Marcus W},
	year = 2010,
	journal = {Proceedings of the National Academy of Sciences},
	publisher = {National Acad Sciences},
	volume = 107,
	number = 4,
	pages = {1452--1457}
}

@inproceedings{louizos2016structured,
	title = {Structured and efficient variational deep learning with matrix gaussian posteriors},
	author = {Louizos, Christos and Welling, Max},
	year = 2016,
	booktitle = {International Conference on Machine Learning},
	pages = {1708--1716}
}

@inproceedings{louizos2017multiplicative,
	title = {Multiplicative Normalizing Flows for Variational Bayesian Neural Networks},
	author = {Louizos, Christos and Welling, Max},
	year = 2017,
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	location = {Sydney, NSW, Australia},
	publisher = {JMLR.org},
	series = {Icml'17},
	pages = {2218--2227},
	url = {http://dl.acm.org/citation.cfm?id=3305890.3305910},
	numpages = 10,
	acmid = 3305910
}

@inproceedings{maas2013rectifier,
	title = {Rectifier nonlinearities improve neural network acoustic models},
	author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
	year = 2013,
	booktitle = {Icml},
	volume = 30,
	number = 1,
	pages = 3
}

@phdthesis{mackay1992bayesian,
	title = {Bayesian methods for adaptive models},
	author = {MacKay, David JC},
	year = 1992,
	school = {California Institute of Technology}
}

@article{mackay1992interp,
	title = {Bayesian interpolation},
	author = {MacKay, David JC},
	year = 1992,
	journal = {Neural computation},
	publisher = {MIT Press},
	volume = 4,
	number = 3,
	pages = {415--447}
}

@article{mackay1992practical,
	title = {A practical Bayesian framework for backpropagation networks},
	author = {MacKay, David JC},
	year = 1992,
	journal = {Neural computation},
	publisher = {MIT Press},
	volume = 4,
	number = 3,
	pages = {448--472}
}

@article{mackay1995probable,
	title = {Probable networks and plausible predictions‚Äîa review of practical Bayesian methods for supervised neural networks},
	author = {MacKay, David JC},
	year = 1995,
	journal = {Network: computation in neural systems},
	publisher = {Taylor \& Francis},
	volume = 6,
	number = 3,
	pages = {469--505}
}

@article{mackay1995review,
	title = {Probable networks and plausible predictions ‚Äî a review of practical Bayesian methods for supervised neural networks},
	author = {David J C Mackay},
	year = 1995,
	journal = {Network: Computation in Neural Systems},
	publisher = {Taylor & Francis},
	volume = 6,
	number = 3,
	pages = {469--505}
}

@book{mackay2003information,
	title = {Information theory, inference and learning algorithms},
	author = {MacKay, David JC and Mac Kay, David JC},
	year = 2003,
	publisher = {Cambridge university press}
}

@article{maddison2016concrete,
	title = {The concrete distribution: A continuous relaxation of discrete random variables},
	author = {Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
	year = 2016,
	journal = {arXiv preprint arXiv:1611.00712}
}

@inproceedings{maddox2019simple,
	title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
	author = {Wesley J. Maddox and Pavel Izmailov and Timur Garipov and Dmitry P. Vetrov and Andrew Gordon Wilson},
	year = 2019,
	booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
	pages = {13132--13143},
	url = {https://proceedings.neurips.cc/paper/2019/hash/118921efba23fc329e6560b27861f0c2-Abstract.html},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/MaddoxIGVW19.bib},
	editor = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
	timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{madey2005agent,
	title = {Agent-Based Scientific Simulation},
	author = {G. Madey and X. Xiang and S. E. Cabaniss and Y. Huang},
	year = 2005,
	month = {jan},
	journal = {Computing in Science \& Engineering},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	volume = 2,
	number = {01},
	pages = {22--29},
	doi = {10.1109/mcse.2005.7},
	issn = {1521-9615},
	keywords = {agents;simulation;scientific simulation;ecosystem}
}

@article{mandt2017stochastic,
	title = {Stochastic Gradient Descent as Approximate Bayesian Inference},
	author = {Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
	journal = {Journal of Machine Learning Research},
	volume = {18},
	pages = {1--35},
	year = {2017}
}

@article{martens21_rapid_train_deep_neural_networ,
	title = {Rapid Training of Deep Neural Networks Without Skip Connections Or Normalization Layers Using Deep Kernel Shaping},
	author = {Martens, James and Ballard, Andy and Desjardins, Guillaume and Swirszcz, Grzegorz and Dalibard, Valentin and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
	year = 2021,
	journal = {CoRR},
	url = {http://arxiv.org/abs/2110.01765v1},
	abstract = {Using an extended and formalized version of the Q/C map analysis of Poole et al. (2016), along with Neural Tangent Kernel theory, we identify the main pathologies present in deep networks that prevent them from training fast and generalizing to unseen data, and show how these can be avoided by carefully controlling the "shape" of the network's initialization-time kernel function.  We then develop a method called Deep Kernel Shaping (DKS), which accomplishes this using a combination of precise parameter initialization, activation function transformations, and small architectural tweaks, all of which preserve the model class. In our experiments we show that DKS enables SGD training of residual networks without normalization layers on Imagenet and CIFAR-10 classification tasks at speeds comparable to standard ResNetV2 and Wide-ResNet models, with only a small decrease in generalization performance. And when using K-FAC as the optimizer, we achieve similar results for networks without skip connections. Our results apply for a large variety of activation functions, including those which traditionally perform very badly, such as the logistic sigmoid. In addition to DKS, we contribute a detailed analysis of skip connections, normalization layers, special activation functions like RELU and SELU, and various initialization schemes, explaining their effectiveness as alternative (and ultimately incomplete) ways of "shaping" the network's initialization-time kernel.},
	archiveprefix = {arXiv},
	eprint = {2110.01765},
	primaryclass = {cs.LG}
}

@techreport{mckinsey2017,
	title = {Smartening up with Artificial Intelligence (AI) - What's in it for Germany and its Industrial Sector?},
	year = 2017,
	month = 4,
	url = {https://www.mckinsey.de/files/170419\_mckinsey\_ki\_final\_m.pdf},
	institution = {McKinsey \& Company, Inc},
	contact = {}
}

@article{melas-kyriazi21_do_you_even_need_atten,
	title = {Do You Even Need Attention? a Stack of Feed-Forward Layers Does Surprisingly Well on Imagenet},
	author = {Melas-Kyriazi, Luke},
	year = 2021,
	journal = {CoRR},
	url = {http://arxiv.org/abs/2105.02723v1},
	abstract = {The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\% top-1 accuracy, compared to 77.9\% and 79.9\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.},
	archiveprefix = {arXiv},
	eprint = {2105.02723},
	primaryclass = {cs.CV}
}

@phdthesis{minka2001family,
	title = {A family of algorithms for approximate Bayesian inference},
	author = {Minka, Thomas Peter},
	year = 2001,
	school = {Massachusetts Institute of Technology}
}

@techreport{minka2005divergence,
	title = {Divergence measures and message passing},
	author = {Minka, Tom and others},
	year = 2005,
	institution = {Technical report, Microsoft Research}
}

@article{mohamed2012acoustic,
	title = {Acoustic modeling using deep belief networks},
	author = {Mohamed, Abdel and Dahl, George E and Hinton, Geoffrey},
	year = 2012,
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	publisher = {Ieee},
	volume = 20,
	number = 1,
	pages = {14--22}
}

@book{murphey2012machine,
	title = {Machine learning, a probabilistic perspective},
	author = {Murphey, kevin},
	year = 2012,
	publisher = {MIT Press},
	address = {Cambridge, MA}
}

@article{mxnet2015,
	title = {MXNet: {A} Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems},
	author = {Tianqi Chen and Mu Li and Yutian Li and Min Lin and Naiyan Wang and Minjie Wang and Tianjun Xiao and Bing Xu and Chiyuan Zhang and Zheng Zhang},
	year = 2015,
	journal = {CoRR},
	volume = {abs/1512.01274},
	url = {http://arxiv.org/abs/1512.01274},
	archiveprefix = {arXiv},
	eprint = {1512.01274},
	timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/ChenLLLWWXXZZ15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{neal2004improving,
	title = {Improving asymptotic variance of MCMC estimators: Non-reversible chains are better},
	author = {Neal, Radford M},
	year = 2004,
	journal = {arXiv preprint math/0407281}
}

@article{neal2011mcmc,
	title = {MCMC using Hamiltonian dynamics},
	author = {Neal, Radford M and others},
	year = 2011,
	journal = {Handbook of markov chain monte carlo},
	volume = 2,
	number = 11,
	pages = 2
}

@book{neal2012bayesian,
	title = {Bayesian learning for neural networks},
	author = {Neal, Radford M},
	year = 2012,
	publisher = {Springer Science \& Business Media},
	volume = 118
}

@inproceedings{netzer2011reading,
	title = {Reading digits in natural images with unsupervised feature learning},
	author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
	year = 2011,
	booktitle = {NIPS workshop on deep learning and unsupervised feature learning},
	volume = 2011,
	number = 2,
	pages = 5
}

@inproceedings{nguyen2015deep,
	title = {Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	year = 2015,
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages = {427--436}
}

@inproceedings{novak2019bayesian,
	title = {Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
	author = {Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
	year = 2019,
	booktitle = {International Conference on Learning Representations},
	url = {https://openreview.net/forum?id=B1g30j0qF7}
}

@article{oh2004gpu,
	title = {GPU implementation of neural networks},
	author = {Oh, Kyoung-Su and Jung, Keechul},
	year = 2004,
	journal = {Pattern Recognition},
	publisher = {Elsevier},
	volume = 37,
	number = 6,
	pages = {1311--1314}
}

@article{olah2017feature,
	title = {Feature Visualization},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = 2017,
	journal = {Distill},
	doi = {10.23915/distill.00007},
	note = {https://distill.pub/2017/feature-visualization}
}

@article{olah2018the,
	title = {The Building Blocks of Interpretability},
	author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
	year = 2018,
	journal = {Distill},
	doi = {10.23915/distill.00010}
}

@online{oord2017,
	title = {WaveNet launches in the Google Assistant},
	author = {A√§ron van den Oord and Tom Walters and Trevor Strohman},
	url = {https://deepmind.com/blog/wavenet-launches-google-assistant/},
	date = {2017-10-04}
}

@article{opper1998bayesian,
	title = {A Bayesian approach to on-line learning},
	author = {Opper, Manfred and Winther, Ole},
	year = 1998,
	journal = {On-line learning in neural networks},
	publisher = {Cambridge university press Cambridge},
	pages = {363--378}
}

@article{opper2009TheVG,
	title = {The Variational Gaussian Approximation Revisited},
	author = {Manfred Opper and C{\'e}dric Archambeau},
	year = 2009,
	journal = {Neural computation},
	volume = {21 3},
	pages = {786--92}
}

@article{opper2009variational,
	title = {The variational Gaussian approximation revisited},
	author = {Opper, Manfred and Archambeau, C{\'e}dric},
	year = 2009,
	journal = {Neural computation},
	publisher = {MIT Press},
	volume = 21,
	number = 3,
	pages = {786--792}
}

@article{osband2016deep,
	title = {Deep Exploration via Bootstrapped {DQN}},
	author = {Ian Osband and Charles Blundell and Alexander Pritzel and Benjamin Van Roy},
	year = 2016,
	journal = {CoRR},
	volume = {abs/1602.04621},
	url = {http://arxiv.org/abs/1602.04621},
	archiveprefix = {arXiv},
	eprint = {1602.04621},
	timestamp = {Mon, 13 Aug 2018 16:46:32 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/OsbandBPR16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paisley2012variational,
	title = {Variational Bayesian inference with stochastic search},
	author = {Paisley, John and Blei, David and Jordan, Michael},
	year = 2012,
	journal = {arXiv preprint arXiv:1206.6430}
}

@inproceedings{pakman2017stochastic,
	title = {Stochastic bouncy particle sampler},
	author = {Pakman, Ari and Gilboa, Dar and Carlson, David and Paninski, Liam},
	year = 2017,
	booktitle = {International Conference on Machine Learning},
	pages = {2741--2750}
}

@article{paszke2017automatic,
	title = {Automatic differentiation in PyTorch},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = 2017
}

@incollection{patterson2013stochastic,
	title = {Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex},
	author = {Patterson, Sam and Teh, Yee Whye},
	year = 2013,
	booktitle = {Advances in Neural Information Processing Systems 26},
	publisher = {Curran Associates, Inc.},
	pages = {3102--3110},
	url = {http://papers.nips.cc/paper/4883-stochastic-gradient-riemannian-langevin-dynamics-on-the-probability-simplex.pdf},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger}
}

@article{pearl2009causal,
	title = {Causal inference in statistics: An overview},
	author = {Pearl, Judea and others},
	year = 2009,
	journal = {Statistics surveys},
	publisher = {The author, under a Creative Commons Attribution License},
	volume = 3,
	pages = {96--146}
}

@book{peters2017,
	title = {Elements of Causal Inference},
	author = {Jonas Peters and Dominik Janzing and Bernhard Scholkopf},
	year = 2017,
	publisher = {MIT Press},
	address = {Cambridge},
	editor = {Francis Bach}
}

@inproceedings{pmlr-v48-amodei16,
	title = {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin},
	author = {Dario Amodei and Sundaram Ananthanarayanan and Rishita Anubhai and Jingliang Bai and Eric Battenberg and Carl Case and Jared Casper and Bryan Catanzaro and Qiang Cheng and Guoliang Chen and Jie Chen and Jingdong Chen and Zhijie Chen and Mike Chrzanowski and Adam Coates and Greg Diamos and Ke Ding and Niandong Du and Erich Elsen and Jesse Engel and Weiwei Fang and Linxi Fan and Christopher Fougner and Liang Gao and Caixia Gong and Awni Hannun and Tony Han and Lappi Johannes and Bing Jiang and Cai Ju and Billy Jun and Patrick LeGresley and Libby Lin and Junjie Liu and Yang Liu and Weigao Li and Xiangang Li and Dongpeng Ma and Sharan Narang and Andrew Ng and Sherjil Ozair and Yiping Peng and Ryan Prenger and Sheng Qian and Zongfeng Quan and Jonathan Raiman and Vinay Rao and Sanjeev Satheesh and David Seetapun and Shubho Sengupta and Kavya Srinet and Anuroop Sriram and Haiyuan Tang and Liliang Tang and Chong Wang and Jidong Wang and Kaifu Wang and Yi Wang and Zhijian Wang and Zhiqian Wang and Shuang Wu and Likai Wei and Bo Xiao and Wen Xie and Yan Xie and Dani Yogatama and Bin Yuan and Jun Zhan and Zhenyao Zhu},
	year = 2016,
	month = {20--22 Jun},
	booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	publisher = {Pmlr},
	address = {New York, New York, USA},
	series = {Proceedings of Machine Learning Research},
	volume = 48,
	pages = {173--182},
	editor = {Maria Florina Balcan and Kilian Q. Weinberger},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech‚Äìtwo vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.}
}

@article{ramanan2018,
	title = {Bounds on Lifting Continuous-State Markov Chains to Speed Up Mixing},
	author = {Ramanan, Kavita and Smith, Aaron},
	year = 2018,
	month = {Sep},
	day = {01},
	journal = {Journal of Theoretical Probability},
	volume = 31,
	number = 3,
	pages = {1647--1678},
	doi = {10.1007/s10959-017-0745-5},
	issn = {1572-9230},
	url = {https://doi.org/10.1007/s10959-017-0745-5},
	abstract = {It is often possible to speed up the mixing of a Markov chain {\$}{\$}{\backslash}{\{} X{\_}{\{}t{\}} {\backslash}{\}}{\_}{\{}t {\backslash}in {\backslash}mathbb {\{}N{\}}{\}}{\$}{\$}{\{}Xt{\}}t‚ààNon a state space {\$}{\$}{\backslash}Omega {\$}{\$}$\Omega$by lifting, that is, running a more efficient Markov chain {\$}{\$}{\backslash}{\{} {\backslash}widehat{\{}X{\}}{\_}{\{}t{\}} {\backslash}{\}}{\_}{\{}t {\backslash}in {\backslash}mathbb {\{}N{\}}{\}}{\$}{\$}{\{}X^t{\}}t‚ààNon a larger state space {\$}{\$}{\backslash}hat{\{}{\backslash}Omega {\}} {\backslash}supset {\backslash}Omega {\$}{\$}$\Omega$^‚äÉ$\Omega$that projects to {\$}{\$}{\backslash}{\{} X{\_}{\{}t{\}} {\backslash}{\}}{\_}{\{}t {\backslash}in {\backslash}mathbb {\{}N{\}}{\}}{\$}{\$}{\{}Xt{\}}t‚ààNin a certain sense. Chen et al. (Proceedings of the 31st annual ACM symposium on theory of computing. ACM, 1999) prove that for Markov chains on finite state spaces, the mixing time of any lift of a Markov chain is at least the square root of the mixing time of the original chain, up to a factor that depends on the stationary measure of {\$}{\$}{\backslash}{\{}X{\_}t{\backslash}{\}}{\_}{\{}t {\backslash}in {\backslash}mathbb {\{}N{\}}{\}}{\$}{\$}{\{}Xt{\}}t‚ààN. Unfortunately, this extra factor makes the bound in Chen et al. (1999) very loose for Markov chains on large state spaces and useless for Markov chains on continuous state spaces. In this paper, we develop an extension of the evolving set method that allows us to refine this extra factor and find bounds for Markov chains on continuous state spaces that are analogous to the bounds in Chen et al. (1999). These bounds also allow us to improve on the bounds in Chen et al. (1999) for some chains on finite state spaces.}
}

@inproceedings{rasmussen2001occam,
	title = {Occam's razor},
	author = {Rasmussen, Carl Edward and Ghahramani, Zoubin},
	year = 2001,
	booktitle = {Advances in neural information processing systems},
	pages = {294--300}
}

@inproceedings{rasmussen2003gaussian,
	title = {Gaussian processes in machine learning},
	author = {Rasmussen, Carl Edward},
	year = 2003,
	booktitle = {Summer School on Machine Learning},
	pages = {63--71},
	organization = {Springer}
}

@inproceedings{redmon2016you,
	title = {You only look once: Unified, real-time object detection},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = 2016,
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {779--788}
}

@book{reichenbach1991direction,
	title = {The direction of time},
	author = {Reichenbach, Hans},
	year = 1991,
	publisher = {Univ of California Press},
	volume = 65
}

@inproceedings{ren2015faster,
	title = {Faster r-cnn: Towards real-time object detection with region proposal networks},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = 2015,
	booktitle = {Advances in neural information processing systems},
	pages = {91--99}
}

@inproceedings{rezende2014stochastic,
	title = {Stochastic backpropagation and approximate inference in deep generative models},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	year = 2014,
	booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
	publisher = {JMLR.org},
	series = {{JMLR} Workshop and Conference Proceedings},
	volume = 32,
	pages = {1278--1286},
	url = {http://proceedings.mlr.press/v32/rezende14.html},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icml/RezendeMW14.bib},
	timestamp = {Wed, 29 May 2019 01:00:00 +0200}
}

@article{rezende2015variational,
	title = {Variational inference with normalizing flows},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	year = 2015,
	journal = {arXiv preprint arXiv:1505.05770}
}

@inproceedings{ribeiro2016should,
	title = {Why should i trust you?: Explaining the predictions of any classifier},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = 2016,
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {1135--1144},
	organization = {Acm}
}

@inproceedings{ridgeway1998interpretable,
	title = {Interpretable Boosted Na{\"\i}ve Bayes Classification.},
	author = {Ridgeway, Greg and Madigan, David and Richardson, Thomas and O'Kane, John},
	year = 1998,
	booktitle = {Kdd},
	pages = {101--104}
}

@article{rosenblatt1958,
	title = {The perceptron: A probabilistic model for information storage and organization in the brain.},
	author = {Rosenblatt, F.},
	year = 1958,
	journal = {Psychological Review},
	volume = 65,
	number = 6,
	pages = {386--408},
	issn = {0033-295x},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	keywords = {probabilistic model, information storage, brain, Brain, Humans, Information Storage and Retrieval, Models, Statistical, Neural Networks (Computer), Perception, Brain, Cognition, Memory, Nervous System}
}

@article{rumelhart1986learning,
	title = {Learning representations by back-propagating errors},
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year = 1986,
	journal = {nature},
	publisher = {Nature Publishing Group},
	volume = 323,
	number = 6088,
	pages = 533
}

@article{russakovsky2015imagenet,
	title = {Imagenet large scale visual recognition challenge},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
	year = 2015,
	journal = {International journal of computer vision},
	publisher = {Springer},
	volume = 115,
	number = 3,
	pages = {211--252}
}

@article{schmidhuber2015deep,
	title = {Deep learning in neural networks: An overview},
	author = {Schmidhuber, J{\"u}rgen},
	year = 2015,
	journal = {Neural networks},
	publisher = {Elsevier},
	volume = 61,
	pages = {85--117}
}

@inproceedings{sermanet2012convolutional,
	title = {Convolutional neural networks applied to house numbers digit classification},
	author = {Pierre Sermanet and Soumith Chintala and Yann LeCun},
	year = 2012,
	booktitle = {Proceedings of the 21st International Conference on Pattern Recognition, {ICPR} 2012, Tsukuba, Japan, November 11-15, 2012},
	publisher = {{IEEE} Computer Society},
	pages = {3288--3291},
	url = {http://ieeexplore.ieee.org/document/6460867/},
	timestamp = {Wed, 16 Oct 2019 14:14:52 +0200},
	biburl = {https://dblp.org/rec/conf/icpr/SermanetCL12.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{silver2017mastering,
	title = {Mastering the game of go without human knowledge},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	year = 2017,
	journal = {Nature},
	publisher = {Nature Publishing Group},
	volume = 550,
	number = 7676,
	pages = 354
}

@article{simonyan2013deep,
	title = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = 2013,
	journal = {CoRR},
	eprint = {1312.6034},
	archiveprefix = {arXiv}
}

@article{simonyan2014very,
	title = {Very deep convolutional networks for large-scale image recognition},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = 2014,
	journal = {CoRR},
	eprint = {1409.1556},
	archiveprefix = {arXiv}
}

@inproceedings{singh2020filter,
	title = {Filter response normalization layer: Eliminating batch dependence in the training of deep neural networks},
	author = {Singh, Saurabh and Krishnan, Shankar},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages = {11237--11246},
	year = {2020}
}

@online{siri2017,
	title = {Deep Learning for Siri's Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis},
	author = {Siri{\ }Team},
	year = 2017,
	month = 8,
	url = {https://machinelearning.apple.com/2017/08/06/siri-voices.html}
}

@article{springenberg2014striving,
	title = {Striving for simplicity: The all convolutional net},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	year = 2014,
	journal = {CoRR},
	eprint = {1412.6806},
	archiveprefix = {arXiv}
}

@article{srivastava2014dropout,
	title = {Dropout: A simple way to prevent neural networks from overfitting},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = 2014,
	journal = {The Journal of Machine Learning Research},
	publisher = {JMLR. org},
	volume = 15,
	number = 1,
	pages = {1929--1958}
}

@book{stein2012interpolation,
	title = {Interpolation of spatial data: some theory for kriging},
	author = {Stein, Michael L},
	year = 2012,
	publisher = {Springer Science \& Business Media}
}

@article{sun2019functional,
	title = {Functional variational bayesian neural networks},
	author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
	year = 2019,
	journal = {arXiv preprint arXiv:1903.05779}
}

@article{sundararajan2017axiomatic,
	title = {Axiomatic attribution for deep networks},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	year = 2017,
	journal = {CoRR},
	eprint = {1703.01365},
	archiveprefix = {arXiv}
}

@inproceedings{szegedy2015going,
	title = {Going deeper with convolutions},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and others},
	year = 2015,
	booktitle = {Cvpr},
	organization = {Cvpr}
}

@article{tabak2010,
	title = {Density estimation by dual ascent of the log-likelihood},
	author = {Tabak, Esteban G. and Vanden-Eijnden, Eric},
	year = 2010,
	month = {03},
	journal = {Commun. Math. Sci.},
	publisher = {International Press of Boston},
	volume = 8,
	number = 1,
	pages = {217--233},
	url = {https://projecteuclid.org:443/euclid.cms/1266935020},
	fjournal = {Communications in Mathematical Sciences}
}

@article{tabak2013family,
	title = {A Family of Nonparametric Density Estimation Algorithms},
	author = {Tabak, E. G. and Turner, Cristina V.},
	year = 2013,
	journal = {Communications on Pure and Applied Mathematics},
	volume = 66,
	number = 2,
	pages = {145--164},
	doi = {10.1002/cpa.21423},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
	abstract = {Abstract A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. ¬© 2012 Wiley Periodicals, Inc.}
}

@article{teh2016consistency,
	title = {Consistency and fluctuations for stochastic gradient Langevin dynamics},
	author = {Teh, Yee Whye and Thiery, Alexandre H and Vollmer, Sebastian J},
	year = 2016,
	journal = {Journal of Machine Learning Research},
	publisher = {Journal of Machine Learning Research},
	volume = 17
}

@article{tensorflow2016,
	title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
	author = {Mart{\'{\i}}n Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Gregory S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian J. Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal J{\'{o}}zefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Man{\'{e}} and Rajat Monga and Sherry Moore and Derek Gordon Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul A. Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda B. Vi{\'{e}}gas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year = 2016,
	journal = {CoRR},
	volume = {abs/1603.04467},
	url = {http://arxiv.org/abs/1603.04467},
	archiveprefix = {arXiv},
	eprint = {1603.04467},
	timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/AbadiABBCCCDDDG16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{tesla2016,
	title = {A Tragic Loss},
	author = {Tesla{\ }Team},
	url = {https://www.tesla.com/en_GB/blog/tragic-loss},
	date = {2016-06-30}
}

@book{theo2006,
	title = {Pattern recognition},
	author = {S. Theodoridis and K. Koutroumbas},
	year = 2006,
	publisher = {Elsevier},
	address = {Amsterdam},
	isbn = {0123695317;9780123695314;},
	edition = 3,
	keywords = {Pattern recognition systems; Applications of Computing; Data Modeling \& Design; COMPUTERS},
	language = {English}
}

@inproceedings{tipping2000relevance,
	title = {The relevance vector machine},
	author = {Tipping, Michael E},
	year = 2000,
	booktitle = {Advances in neural information processing systems},
	pages = {652--658}
}

@article{tipping2001,
	title = {Sparse Bayesian learning and the relevance vector machine},
	author = {Tipping, Michael E},
	year = 2001,
	journal = {Journal of machine learning research},
	volume = 1,
	pages = {211--244}
}

@inproceedings{tishby1989,
	title = {Consistent inference of probabilities in layered networks: predictions and generalizations},
	author = {N. Tishby and E. Levin and S. A. Solla},
	year = 1989,
	booktitle = {International 1989 Joint Conference on Neural Networks},
	volume = {},
	number = {},
	pages = {403--409 vol.2},
	doi = {10.1109/ijcnn.1989.118274},
	issn = {},
	keywords = {learning systems;neural nets;probability;Gibbs distribution;consistency condition;error minimization;general input-output relation;layered networks;learning;learning curves;neural network;optimal architecture;probabilities;statistical formalism;statistical framework;training set;training size;Learning systems;Neural networks;Probability}
}

@inproceedings{titsias2009variational,
	title = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
	author = {Michalis Titsias},
	year = 2009,
	month = {16--18 Apr},
	booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
	publisher = {Pmlr},
	address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
	series = {Proceedings of Machine Learning Research},
	volume = 5,
	pages = {567--574},
	url = {http://proceedings.mlr.press/v5/titsias09a.html},
	editor = {David van Dyk and Max Welling},
	pdf = {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.}
}

@article{tolstikhin21_mlp_mixer,
	title = {Mlp-Mixer: an All-Mlp Architecture for Vision},
	author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	year = 2021,
	journal = {CoRR},
	url = {http://arxiv.org/abs/2105.01601v4},
	abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision.  Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).  MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
	archiveprefix = {arXiv},
	eprint = {2105.01601v4},
	primaryclass = {cs.CV}
}

@article{touvron21_resml,
	title = {Resmlp: Feedforward Networks for Image Classification With Data-Efficient Training},
	author = {Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Izacard, Gautier and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
	year = 2021,
	journal = {CoRR},
	url = {http://arxiv.org/abs/2105.03404v2},
	abstract = {We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.},
	archiveprefix = {arXiv},
	eprint = {2105.03404},
	primaryclass = {cs.CV}
}

@incollection{turner2011two,
	title = {Two problems with variational expectation maximisation for time-series models},
	author = {Turner, R. E. and Sahani, M.},
	year = 2011,
	booktitle = {Bayesian Time Series Models},
	publisher = {Cambridge University Press},
	editor = {Barber, D. and Cemgil, A. T. and Chiappa, S.}
}

@techreport{van2017,
	title = {Artificial intelligence in wealth and asset management},
	author = {Edgar Van Tuyll Van Serooskerken},
	year = 2017,
	month = 1,
	url = {https://perspectives.pictet.com/wp-content/uploads/2016/12/Edgar-van-Tuyll-van-Serooskerken-Pictet-Report-winter-2016-2.pdf},
	institution = {Pictet on Robot Advisors},
	contact = {}
}

@inproceedings{van2017convolutional,
	title = {Convolutional gaussian processes},
	author = {Van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
	year = 2017,
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {2849--2858}
}

@article{vapnik1999overview,
	title = {An overview of statistical learning theory},
	author = {Vapnik, Vladimir Naumovich},
	year = 1999,
	journal = {IEEE transactions on neural networks},
	publisher = {Ieee},
	volume = 10,
	number = 5,
	pages = {988--999}
}

@book{vapnik2000,
	title = {The Nature of Statistical Learning Theory},
	author = {Vapnik, Vladimir, N.},
	year = 2000,
	publisher = {Springer},
	address = {Ny},
	edition = 2
}

@inproceedings{vladimirova2019understanding,
	title = {Understanding priors in bayesian neural networks at the unit level},
	author = {Vladimirova, Mariia and Verbeek, Jakob and Mesejo, Pablo and Arbel, Julyan},
	year = 2019,
	booktitle = {International Conference on Machine Learning},
	pages = {6458--6467},
	organization = {PMLR}
}

@article{vollmer2015non,
	title = {(Non-) asymptotic properties of stochastic gradient Langevin dynamics},
	author = {Vollmer, Sebastian J and Zygalakis, Konstantinos C and others},
	year = 2015,
	journal = {arXiv preprint arXiv:1501.00438}
}

@article{vu2018,
	title = {A Shared Vision for Machine Learning in Neuroscience},
	author = {Vu,MAT and Adali,T. and Ba,D. and Buzsaki,G. and Carlson,D. and Heller,K. and Liston,C. and Rudin,C. and Sohal,VS and Widge,AS and Mayberg,HS and Sapiro,G. and Dzirasa,K.},
	year = 2018,
	journal = {Journal Of Neuroscience},
	volume = 38,
	number = 7,
	pages = {1601--1607},
	abstract = {With ever-increasing advancements in technology, neuroscientists are able to collect data in greater volumes and with finer resolution. The bottleneck in understanding how the brain works is consequently shifting away from the amount and type of data we can collect and toward what we actually do with the data. There has been a growing interest in leveraging this vast volume of data across levels of analysis, measurement techniques, and experimental paradigms to gain more insight into brain function. Such efforts are visible at an international scale, with the emergence of big data neuroscience initiatives, such as the BRAIN initiative (Bargmann et al., 2014), the Human Brain Project, the Human Connectome Project, and the National Institute of Mental Health's Research Domain Criteria initiative. With these large-scale projects, much thought has been given to data-sharing across groups (Poldrack and Gorgolewski, 2014; Sejnowski et al., 2014); however, even with such data-sharing initiatives, funding mechanisms, and infrastructure, there still exists the challenge of how to cohesively integrate all the data. At multiple stages and levels of neuroscience investigation, machine learning holds great promise as an addition to the arsenal of analysis tools for discovering how the brain works.},
	keywords = {explainable artificial intelligence; DEPRESSION; STIMULATION; BRAIN IMAGES; machine learning; NEUROSCIENCES; reinforcement learning; INDEPENDENT COMPONENT ANALYSIS; MEMORY; BIG DATA; CORTEX; REINFORCEMENT; OPTIMIZATION; REGISTRATION},
	language = {English}
}

@article{vucelja2016lifting,
	title = {Lifting‚Äîa nonreversible Markov chain Monte Carlo algorithm},
	author = {Vucelja, Marija},
	year = 2016,
	journal = {American Journal of Physics},
	publisher = {Aapt},
	volume = 84,
	number = 12,
	pages = {958--968}
}

@book{wainwright2008graphical,
	title = {Graphical models, exponential families, and variational inference},
	author = {Wainwright, Martin J and Jordan, Michael Irwin},
	year = 2008,
	publisher = {Now Publishers Inc}
}

@online{wakefield2016,
	title = {Microsoft chatbot is taught to swear on Twitter},
	author = {Jane Wakefield},
	url = {www.bbc.com/news/technology-35890188},
	date = {2016-05-04},
	editor = {{BBC News}}
}

@inproceedings{wang2005inadequacy,
	title = {Inadequacy of interval estimates corresponding to variational Bayesian approximations.},
	author = {Wang, Bo and Titterington, DM},
	year = 2005,
	booktitle = {Aistats},
	organization = {Barbados}
}

@inproceedings{wang2013fast,
	title = {Fast dropout training},
	author = {Wang, Sida and Manning, Christopher},
	year = 2013,
	booktitle = {international conference on machine learning},
	pages = {118--126}
}

@inproceedings{wang2016,
	title = {Bayesian Rule Sets for Interpretable Classification},
	author = {T. Wang and C. Rudin and F. Velez-Doshi and Y. Liu and E. Klampfl and P. MacNeille},
	year = 2016,
	month = {Dec},
	booktitle = {2016 IEEE 16th International Conference on Data Mining (ICDM)},
	volume = {},
	number = {},
	pages = {1269--1274},
	doi = {10.1109/icdm.2016.0171},
	issn = {},
	keywords = {Bayes methods;inference mechanisms;learning (artificial intelligence);pattern classification;Bayesian rule sets;MAP;UCI data sets;domain-specific interpretability definition;inference approach;interpretable classification;learning rule set models;Bayes methods;Computational modeling;Data mining;Data models;Predictive models;Search problems;Simulated annealing;Bayesian modeling;association rules;classifier;interpretable machine learning}
}

@article{wang2016towards,
	title = {Towards bayesian deep learning: A survey},
	author = {Wang, Hao and Yeung, Dit-Yan},
	year = 2016,
	journal = {arXiv preprint arXiv:1604.01662}
}

@article{warburg20_bayes_tripl_loss,
	title = {Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval},
	author = {Warburg, Frederik and J{\o}rgensen, Martin and Civera, Javier and Hauberg, S{\o}ren},
	year = 2020,
	journal = {CoRR},
	url = {http://arxiv.org/abs/2011.12663v2},
	abstract = {Uncertainty quantification in image retrieval is crucial for downstream decisions, yet it remains a challenging and largely unexplored problem. Current methods for estimating uncertainties are poorly calibrated, computationally expensive, or based on heuristics. We present a new method that views image embeddings as stochastic features rather than deterministic features. Our two main contributions are (1) a likelihood that matches the triplet constraint and that evaluates the probability of an anchor being closer to a positive than a negative; and (2) a prior over the feature space that justifies the conventional l2 normalization. To ensure computational efficiency, we derive a variational approximation of the posterior, called the Bayesian triplet loss, that produces state-of-the-art uncertainty estimates and matches the predictive performance of current state-of-the-art methods.},
	archiveprefix = {arXiv},
	eprint = {2011.12663},
	primaryclass = {cs.CV}
}

@inproceedings{welling2011bayesian,
	title = {Bayesian learning via stochastic gradient Langevin dynamics},
	author = {Welling, Max and Teh, Yee W},
	year = 2011,
	booktitle = {Proceedings of the 28th international conference on machine learning (ICML-11)},
	pages = {681--688}
}

@inproceedings{wen2018flipout,
	title = {Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches},
	author = {Yeming Wen and Paul Vicol and Jimmy Ba and Dustin Tran and Roger B. Grosse},
	year = 2018,
	booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher = {OpenReview.net},
	url = {https://openreview.net/forum?id=rJNpifWAb},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/iclr/WenVBTG18.bib},
	timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{wenzel2020good,
	author = {Florian Wenzel and Kevin Roth and Bastiaan S. Veeling and Jakub Swiatkowski and Linh Tran and Stephan Mandt and Jasper Snoek and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},
	title = {How Good is the Bayes Posterior in Deep Neural Networks Really?},
	booktitle = {International Conference of Machine Learning},
	year = {2020}
}

@inproceedings{williams1997computing,
	title = {Computing with infinite networks},
	author = {Williams, Christopher KI},
	year = 1997,
	booktitle = {Advances in neural information processing systems},
	pages = {295--301}
}

@book{williamson2005,
	title = {Bayesian Nets and Causality},
	author = {Williamson, Jon},
	year = 2005,
	publisher = {Oxford University Press},
	address = {Oxford}
}

@article{wilson1984,
	title = {Variance Reduction Techniques for Digital Simulation},
	author = {James R. Wilson},
	year = 1984,
	journal = {American Journal of Mathematical and Management Sciences},
	publisher = {Taylor & Francis},
	volume = 4,
	number = {3-4},
	pages = {277--312},
	doi = {10.1080/01966324.1984.10737147},
	url = {https://doi.org/10.1080/01966324.1984.10737147},
	eprint = {https://doi.org/10.1080/01966324.1984.10737147}
}

@article{wilson2020bayesian,
	title = {Bayesian deep learning and a probabilistic perspective of generalization},
	author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	year = 2020,
	journal = {arXiv preprint arXiv:2002.08791}
}

@article{wu2017generalized,
	title = {Generalized bouncy particle sampler},
	author = {Wu, Changye and Robert, Christian P},
	year = 2017,
	journal = {arXiv preprint arXiv:1706.04781}
}

@article{xiao2017fashion,
	title = {Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	year = 2017,
	journal = {arXiv preprint arXiv:1708.07747}
}

@article{yosinski2015understanding,
	title = {Understanding neural networks through deep visualization},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	year = 2015,
	journal = {CoRR},
	eprint = {1506.06579},
	archiveprefix = {arXiv}
}

@inproceedings{zeiler2011,
	title = {Adaptive deconvolutional networks for mid and high level feature learning},
	author = {M. D. Zeiler and G. W. Taylor and R. Fergus},
	year = 2011,
	month = {Nov},
	booktitle = {2011 International Conference on Computer Vision},
	volume = {},
	number = {},
	pages = {2018--2025},
	doi = {10.1109/iccv.2011.6126474},
	issn = {1550-5499},
	keywords = {deconvolution;feature extraction;image classification;image representation;inference mechanisms;learning (artificial intelligence);Caltech-101 datasets;Caltech-256 datasets;adaptive deconvolutional networks;classifier;complete objects;convolutional sparse coding;feature extraction;hierarchical model;high level feature learning;high-level object parts;image decompositions;inference scheme;low-level edges;max pooling;mid level feature learning;mid-level edge junctions;natural images;Adaptation models;Computational modeling;Deconvolution;Image reconstruction;Mathematical model;Switches;Training}
}

@inproceedings{zeiler2013stochastic,
	title = {Stochastic Pooling for Regularization of Deep Convolutional Neural Networks},
	author = {Matthew D. Zeiler and Rob Fergus},
	year = 2013,
	booktitle = {1st International Conference on Learning Representations, {ICLR} 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Conference Track Proceedings},
	url = {http://arxiv.org/abs/1301.3557},
	editor = {Yoshua Bengio and Yann LeCun},
	timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1301-3557.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zeiler2014visualizing,
	title = {Visualizing and understanding convolutional networks},
	author = {Zeiler, Matthew D and Fergus, Rob},
	year = 2014,
	booktitle = {European conference on computer vision},
	pages = {818--833},
	organization = {Springer}
}

@Comment{
Local Variables:
bibtex-dialect: biblatex
End:
}

